{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "practical-contents",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "complete-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset_functions import *\n",
    "from common_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-transaction",
   "metadata": {},
   "source": [
    "# NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-anime",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-genre",
   "metadata": {},
   "source": [
    "### ARQUITECTURE OF NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "careful-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_size, hidden_layer, num_labels):\n",
    "    \"\"\"\n",
    "    A partir de tres números enteros (entrada, capas ocultas y etiquetas)\n",
    "    genera una arquitectura de red neuronal con los valores de theta\n",
    "    aleatorios.\n",
    "    Devuelve un único array con todas las theta\n",
    "    \"\"\"\n",
    "    eIni = 0.12\n",
    "\n",
    "    Theta1_sh = (hidden_layer, input_size + 1)\n",
    "    Theta2_sh = (num_labels, hidden_layer + 1)\n",
    "\n",
    "    thetas_random = dict()\n",
    "\n",
    "    thetas_random[\"Theta1\"] = random_thetas(Theta1_sh, eIni)\n",
    "    thetas_random[\"Theta2\"] = random_thetas(Theta2_sh, eIni)\n",
    "\n",
    "    th_random = np.concatenate(\n",
    "        (\n",
    "            np.ravel(thetas_random[\"Theta1\"]),\n",
    "            np.ravel(thetas_random[\"Theta2\"])\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return th_random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-castle",
   "metadata": {},
   "source": [
    "### COST AND GRADIENT FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "connected-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coste(X, Y, w, lam):\n",
    "    \"\"\"\n",
    "    Función de coste, recibe como entrada las características,\n",
    "    los resultados, los pesos (las thetas) y el parámetro lambda\n",
    "    de regularización.\n",
    "    Devuelve el coste regularizado\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    J_theta = 0    \n",
    "    aux = 0\n",
    "    _, _, _, _, h_theta = forward_prop(X, w)\n",
    "    \n",
    "    for i in range(m):\n",
    "        aux +=np.sum(-Y[i] * np.log(h_theta[i])\n",
    "                     - (1 - Y[i])* np.log(1 - h_theta[i]))\n",
    "            \n",
    "    \n",
    "    J_theta = (1 / m) * aux\n",
    "    \n",
    "    # Regularizacion\n",
    "    reg = 0\n",
    "\n",
    "    for theta in w.keys():\n",
    "        if \"__\" not in theta:\n",
    "            reg += np.sum(np.square(w[theta][:, 1:]))\n",
    "\n",
    "    reg *= (lam /(2 * m))\n",
    "\n",
    "    # Coste Regularizado\n",
    "    J_theta += reg\n",
    "    \n",
    "    return J_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "knowing-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente(X, Y, w, lam):\n",
    "    \"\"\"\n",
    "    Función de gradiente, recibe como entrada las características,\n",
    "    los resultados, los pesos (las thetas) y el parámetro lambda\n",
    "    de regularización.\n",
    "    Devuelve la gradiente regularizada\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    d = dict()\n",
    "    d[\"delta1\"] = np.zeros(w[\"Theta1\"].shape)\n",
    "\n",
    "    d[\"delta2\"] = np.zeros(w[\"Theta2\"].shape)\n",
    "    a1, z2, a2, z3, h_theta = forward_prop(X, w)\n",
    "\n",
    "    for i in range(m):\n",
    "        # Calcular d2 y d3\n",
    "        d3 = h_theta[i] - Y[i]                  # (10, )\n",
    "        g_z2 = a2[i] * (1 - a2[i])              # (26, )\n",
    "        d2 = np.dot(d3, w[\"Theta2\"]) * g_z2     # (26, )\n",
    "        #d2 = d2[1:]                             # (25, )\n",
    "    \n",
    "        # Actualizar deltas\n",
    "        d[\"delta1\"] += np.dot(d2[1:, np.newaxis], a1[i][np.newaxis, :])\n",
    "        d[\"delta2\"] += np.dot(d3[:, np.newaxis], a2[i][np.newaxis, :])\n",
    "\n",
    "    d[\"delta1\"] /= m\n",
    "    d[\"delta2\"] /= m\n",
    "    \n",
    "    #Regularizar deltas\n",
    "    reg1 = ((lam / m) * w[\"Theta1\"][:, 1:]) # No theta primera columna\n",
    "    reg2 = ((lam / m) * w[\"Theta2\"][:, 1:]) # No theta primera columna\n",
    "    d[\"delta1\"][:, 1:] += reg1              # j = 0 no tiene regularización\n",
    "    d[\"delta2\"][:, 1:] += reg2              # j = 0 no tiene regularización\n",
    "\n",
    "    \n",
    "    return np.concatenate(\n",
    "        (np.ravel(d[\"delta1\"]),\n",
    "         np.ravel(d[\"delta2\"]))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-ranch",
   "metadata": {},
   "source": [
    "### NEURAL NETWORK FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "split-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, w):\n",
    "    \"\"\"\n",
    "    Función de propagación hacia adelante, aplica la lógica de las\n",
    "    Redes Neuronales para únicamente dos theta.\n",
    "    \"\"\"\n",
    "    a1 = X\n",
    "    a1 = np.hstack([np.ones([X.shape[0], 1]), a1])\n",
    "\n",
    "    z2 = np.dot(w['Theta1'], a1.T)\n",
    "    a2 = sigmoid(z2).T\n",
    "    a2 = np.hstack([np.ones([a2.shape[0], 1]), a2])\n",
    "\n",
    "    z3 = np.dot(w['Theta2'], a2.T)\n",
    "    h = sigmoid(z3).T\n",
    "\n",
    "    return a1, z2, a2, z3, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "affected-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(params_rn, num_entradas, num_ocultas, num_etiquetas, X, Y, reg):\n",
    "    \"\"\"\n",
    "    Esta función devuelve una tupla (coste, gradiente) con el coste y\n",
    "    el gradiente de una red neuronal de tres capas, con num_entradas,\n",
    "    num_ocultas nodos en la capa oculta y num_etiquetas nodos en la\n",
    "    capa de salida. Si m es el número de ejemplos de entrenamiento,\n",
    "    la dimensiónde de 'X' es (m, num_entradas) y la de 'y'\n",
    "    es (m, num_etiquetas)\n",
    "    \"\"\"\n",
    "    w = dict()\n",
    "    w[\"Theta1\"], w[\"Theta2\"] = relocate(params_rn, num_entradas, num_ocultas, num_etiquetas)\n",
    "    \n",
    "    return coste(X, Y, w, reg), gradiente(X, Y, w, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-animation",
   "metadata": {},
   "source": [
    "### SUPPORT FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "korean-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relocate(params_rn, num_entradas, num_ocultas, num_etiquetas):\n",
    "    \"\"\"\n",
    "    A partir de un único vector y el número de entradas, de nodos ocultos\n",
    "    y etiquetas se devuelven los vectores Theta1 y Theta2, con los tamaños\n",
    "    establecidos\n",
    "    \"\"\"\n",
    "    Theta1 = np.reshape(params_rn[:num_ocultas * (num_entradas + 1)],\n",
    "                        (num_ocultas, (num_entradas + 1)))\n",
    "    Theta2 = np.reshape(params_rn[num_ocultas * (num_entradas + 1):],\n",
    "                        (num_etiquetas, (num_ocultas + 1)))\n",
    "    return Theta1, Theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "appropriate-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_thetas(shape, E):\n",
    "    \"\"\"\n",
    "    Recibe como parámetros las dimensiones y el Epsilon (rango -> [-E, E])\n",
    "    \n",
    "    Primero creamos una matriz de positivos y negativos\n",
    "    Posteriormente creamos una matriz con números aleatorios positivos < Epsilon\n",
    "    Multiplicamos las dos matrices, tenemos aleatorios positivos y negativos.\n",
    "    \n",
    "    Devuelve las dimensiones con valores aleatorios\n",
    "\n",
    "    \"\"\"\n",
    "    posNeg = np.random.random((shape))\n",
    "    pos = np.where(posNeg < .5)\n",
    "    neg = np.where(posNeg >= .5)\n",
    "    posNeg[pos] = 1\n",
    "    posNeg[neg] = -1\n",
    "        \n",
    "    return (np.random.random((shape)) %E ) * posNeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-biotechnology",
   "metadata": {},
   "source": [
    "### TRAIN NEURAL NETWORK FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "graduate-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(backprop, thetas, X, Y, input_size, hidden_layer,\n",
    "             num_labels, reg = 1, iterations = 70):\n",
    "    \"\"\"\n",
    "    Función que, a partir de la función de backprop, las características,\n",
    "    los resultados, los tamaños de las capas, la regularización y las\n",
    "    iteraciones, entrena la red neuronal para obtener los parámetros theta\n",
    "    óptimos.\n",
    "    Devuelve las theta óptimas en forma de diccionario\n",
    "    \"\"\"\n",
    "    \n",
    "    # Thetas optimas\n",
    "    res = opt.minimize(\n",
    "        fun = backprop,\n",
    "        x0 = thetas,\n",
    "        args=(input_size, hidden_layer, num_labels, X, Y, reg),\n",
    "        options={'maxiter': iterations},\n",
    "        method='TNC',\n",
    "        jac=True\n",
    "    )\n",
    "\n",
    "    # Recolocamos\n",
    "    w = dict()\n",
    "    w[\"Theta1\"], w[\"Theta2\"] = relocate(res.x,\n",
    "                                         X.shape[1],\n",
    "                                         hidden_layer,\n",
    "                                         num_labels)\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-incentive",
   "metadata": {},
   "source": [
    "### PREDICTION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "distinguished-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    \"\"\"\n",
    "    Función que a partir de las características y los pesos (thetas)\n",
    "    predice los resultados de dichas características.\n",
    "    Devuelve un array con esas predicciones\n",
    "    \"\"\"\n",
    "    Y_hat = []\n",
    "    _, _, _, _, pred = forward_prop(X, w)\n",
    "    \n",
    "    for i in range(pred.shape[0]):\n",
    "        ejemplo = pred[i]\n",
    "        num = np.argmax(ejemplo)\n",
    "        Y_hat.append(num)\n",
    "    \n",
    "    Y_hat = np.array(Y_hat)\n",
    "    \n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "second-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(X, Y, w):\n",
    "    \"\"\"\n",
    "    Función que a partir de las características, los resultados\n",
    "    y los pesos (thetas), calcula el porcentaje de acierto del clasificador.\n",
    "    Devuelve un float con el porcentaje de acierto.\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]\n",
    "    \n",
    "    Y_hat = predict(X, w)\n",
    "    \n",
    "    percentage = np.round(\n",
    "        np.sum(Y_hat == Y.argmax(1)) / m * 100,\n",
    "        decimals = 2\n",
    "    )\n",
    "    \n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-velvet",
   "metadata": {},
   "source": [
    "### PLOT ITERATIONS / COST FIGURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "connected-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_opt_lambdas(iterations, hit_history, lambdas):\n",
    "    \"\"\"\n",
    "    Función que imprime las iteraciones en el eje X,\n",
    "    y la precisión en el eje Y\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "\n",
    "    for hit in hit_history:\n",
    "        plt.plot(iterations, hit)\n",
    "\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('hit_accuracy')\n",
    "    plt.legend(lambdas)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-worth",
   "metadata": {},
   "source": [
    "### CHOOSE OPTIMAL VALUES FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "stuck-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opt_thetas(th_random, X_train, Y_train, X_test, Y_test,\n",
    "                    input_size, hidden_layer, num_labels):\n",
    "    \"\"\"\n",
    "    Función que recibe unas theta aleatorias, las características y\n",
    "    los resultados, tanto de train set como de test set, y el número de\n",
    "    capas.\n",
    "    Prueba, a partir de unas iteraciones y unos parámetros de\n",
    "    regularización preestablecidos, todas las combinaciones, \n",
    "    de esta forma obtiene las theta óptimas (basándonos en la tasa de\n",
    "    acierto que tiene)\n",
    "    Devuelve la precisión, las thetas optimas y su combinación óptima\n",
    "    de parámetros\n",
    "    \"\"\"\n",
    "    iterations = [ 10, 50, 100, 200, 300 ]\n",
    "    lambdas = [ 0.01, 0.03, 0.1, 0.3, 1, 3, 10 ]\n",
    "#     iterations = [ 1,2,3 ]\n",
    "#     lambdas = [ 0.01, 0.03, 0.1, 0.3 ]\n",
    "    \n",
    "    precision_opt = -1\n",
    "    thetas_opt = np.array(0)\n",
    "    it_opt = iterations[0]\n",
    "    lambd_opt = lambdas[0]\n",
    "    \n",
    "    hit_lambda_history = []\n",
    "    hit_total_history = []\n",
    "\n",
    "    for lambd in lambdas:\n",
    "        hit_lambda_history = []\n",
    "        \n",
    "        for i in iterations:\n",
    "            thetas = train(backprop, th_random, X_train, Y_train,\n",
    "                              input_size, hidden_layer, num_labels,\n",
    "                              lambd, i)\n",
    "\n",
    "            p = acc(X_test, Y_test, thetas)\n",
    "            hit_lambda_history.append(p)\n",
    "            \n",
    "            print(\"Lambda: {}\".format(lambd),\n",
    "                  \"\\tIteraciones: {}\".format(i),\n",
    "                  \"\\tPrecisión:{}%\\n\".format(p)\n",
    "                 )\n",
    "            \n",
    "            if(p > precision_opt):\n",
    "                precision_opt = p\n",
    "                thetas_opt = thetas\n",
    "                it_opt = i\n",
    "                lambd_opt = lambd\n",
    "                \n",
    "            if(precision_opt == 100.0):\n",
    "                break\n",
    "                \n",
    "\n",
    "        hit_total_history.append(np.array(hit_lambda_history))\n",
    "        \n",
    "        if(precision_opt == 100.0):\n",
    "            break\n",
    "\n",
    "    if(precision_opt < 100.0):\n",
    "        print_opt_lambdas(iterations, hit_total_history, lambdas)\n",
    "\n",
    "    return precision_opt, thetas_opt, it_opt, lambd_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-cycling",
   "metadata": {},
   "source": [
    "### STUDY BIAS AND VARIANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ordered-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_errors(th_random,\n",
    "                   num_entradas, num_ocultas, num_etiquetas,\n",
    "                   X_train, Y_train, X_cv, Y_cv, lambd = 0,\n",
    "                   iterations = 100, max_examples = 100\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    A partir de unas theta aleatorias, las capas, las características\n",
    "    y los resultados, tanto del train set como de la CV, la regularización,\n",
    "    las iteraciones, y el número máximo de ejemplos a probar, calcula la\n",
    "    tasa de error del trainset y de la CV set.\n",
    "    Devuelve los vectores de error del entrenamiento y la CV\n",
    "    \"\"\"\n",
    "    \n",
    "    trainError = []\n",
    "    cvError = []\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    for i in range(1, max_examples):\n",
    "#         print(i, \" de \", max_examples)\n",
    "        X_i = X_train[0:i]\n",
    "        Y_i = Y_train[0:i]\n",
    "\n",
    "        res = opt.minimize(\n",
    "            fun = backprop,\n",
    "            x0 = th_random,\n",
    "            args=(num_entradas, num_ocultas, num_etiquetas,\n",
    "                  X_i, Y_i, lambd),\n",
    "            method='TNC',\n",
    "            jac=True,\n",
    "            options={'maxiter': iterations},\n",
    "        )\n",
    "\n",
    "        trainError.append(backprop(res.x,\n",
    "                                   num_entradas, num_ocultas, num_etiquetas,\n",
    "                                   X_i, Y_i, lambd)\n",
    "                          [0])\n",
    "        cvError.append(backprop(res.x,\n",
    "                                num_entradas, num_ocultas, num_etiquetas,\n",
    "                                X_cv, Y_cv, lambd)\n",
    "                       [0])\n",
    "        \n",
    "    return np.array(trainError), np.array(cvError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "opened-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_learning_errors(train_error, cv_error):\n",
    "    \"\"\"\n",
    "    Dibuja el error del entrenamiento y de la CV\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    r = range(0, len(train_error))\n",
    "\n",
    "    plt.plot(r, train_error)\n",
    "    plt.plot(r, cv_error)\n",
    "    plt.legend(['Train', 'Cross Validation'])\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Cost')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-present",
   "metadata": {},
   "source": [
    "## EXTERNAL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "overall-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nn_model(thetas):\n",
    "    \"\"\"\n",
    "    Guarda dos array de thetas en la ruta models,\n",
    "    con los archivos theta1_nn.npy  y  theta2_nn.npy\n",
    "    \"\"\"\n",
    "    np.save(\"models/theta1_nn.npy\", thetas[\"Theta1\"])\n",
    "    np.save(\"models/theta2_nn.npy\", thetas[\"Theta2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "medieval-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nn_model():\n",
    "    \"\"\"\n",
    "    Carga dos array de thetas en la ruta models,\n",
    "    con los archivos theta1_nn.npy  y  theta2_nn.npy\n",
    "    y los devuelve en un único diccionario\n",
    "    \"\"\"\n",
    "    thetas = dict()\n",
    "    \n",
    "    thetas[\"Theta1\"] = np.load(\"models/theta1_nn.npy\")\n",
    "    thetas[\"Theta2\"] = np.load(\"models/theta2_nn.npy\")\n",
    "        \n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "assisted-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_nn_prediction():\n",
    "    \"\"\"\n",
    "    Carga los datos y las thetas óptimas, divide los datos y\n",
    "    prueba las thetas óptimas sobre esos datos.\n",
    "    Finalmente muestra el porcentaje de acierto de esos datos\n",
    "    \"\"\"\n",
    "    X, Y = read_dataset()\n",
    "    X, Y = manage_data(X, Y, use_onehot = True)\n",
    "    _, _, _, _, X_test, Y_test = divide_dataset(X, Y)\n",
    "    w = load_nn_model()\n",
    "    \n",
    "    print(\"The neural network is reliable in {:.2f}% of the time\\n\"\n",
    "      .format(acc(X_test, Y_test, w)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "incredible-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_example_nn(example):\n",
    "    \"\"\"\n",
    "    Carga las theta óptimas y, a partir de un ejemplo, predice su resultado\n",
    "    devuelve la predicción como booleano\n",
    "    \"\"\"\n",
    "    w = load_nn_model()\n",
    "    _, _, _, _, prediction = forward_prop(np.array([example]), w)\n",
    "    \n",
    "    return bool(prediction.argmin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "constitutional-maker",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main_nn():\n",
    "    \"\"\"\n",
    "    Función que entrena el clasificador, obtiene las theta óptimas y\n",
    "    guarda el modelo óptimo.\n",
    "    \"\"\"\n",
    "    X, Y = read_dataset()\n",
    "    X, Y = manage_data(X, Y, use_onehot = True)\n",
    "\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "\n",
    "    X_train, Y_train, X_cv, Y_cv, X_test, Y_test = divide_dataset(X, Y)\n",
    "    \n",
    "    input_size = X_train.shape[1]\n",
    "    num_labels = 2\n",
    "\n",
    "    hidden_layer_nodes = [ 10, 25, 50, 75, 100, 150, 200 ]\n",
    "\n",
    "    acc_opt = 0\n",
    "    thetas_opt = np.array(0)\n",
    "    th_random_opt = np.array(0)\n",
    "    it_opt = np.inf\n",
    "    lambd_opt = np.inf\n",
    "    hidden_layer_opt = hidden_layer_nodes[0]\n",
    "\n",
    "    for nodes in hidden_layer_nodes: \n",
    "\n",
    "        th_random = model(input_size, nodes, num_labels)\n",
    "\n",
    "        acc_act, thetas_act, it_act, lambd_act = get_opt_thetas(\n",
    "            th_random,\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            X_test,\n",
    "            Y_test,\n",
    "            input_size,\n",
    "            nodes,\n",
    "            num_labels)\n",
    "\n",
    "        if(acc_act > acc_opt):\n",
    "            acc_opt = acc_act\n",
    "            thetas_opt = thetas_act\n",
    "            it_opt = it_act\n",
    "            lambd_opt = lambd_act\n",
    "            hidden_layer_opt = nodes\n",
    "\n",
    "        if(acc_act == 100.0):\n",
    "            break\n",
    "        else:\n",
    "            train_error, cv_error = learning_errors(np.concatenate((np.ravel(thetas_opt[\"Theta1\"]), np.ravel(thetas_opt[\"Theta2\"]))),\n",
    "                    input_size, hidden_layer_opt, num_labels,\n",
    "                    X_train, Y_train, X_cv, Y_cv,\n",
    "                    lambd = lambd_opt, iterations = it_opt)\n",
    "\n",
    "            print_learning_errors(train_error, cv_error)\n",
    "        \n",
    "    save_nn_model(thetas_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "temporal-indianapolis",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# main_nn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
